<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Beacon3D: A Benchmark for 3D Vision-Language Understanding">
  <meta name="keywords" content="3D Vision-Language, 3D Scene Understanding, Benchmark, Multi-modal Model">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Beacon3D</title>

  <script>
    // functions borrowed from PaLM-E
    timeoutIds = [];
        
    function populateDemo(img) {
      // console.log("img", img)

      var img1 = document.getElementById("obj_1");
      img1.onclick = null;

      var img2 = document.getElementById("obj_2");
      img2.onclick = null;

      var img3 = document.getElementById("obj_3");
      img3.onclick = null;

      for (const idx of [1,2,3]) {
        var g_text = document.getElementById("g"+idx);
        var q_text = document.getElementById("q"+idx);
        var a_text = document.getElementById("a"+idx);
        g_text.innerHTML = "";
        q_text.innerHTML = "";
        a_text.innerHTML = "";
      }

      model1 = scene1.getObjectByName("mesh")
      scene1.remove(model1)
      document.querySelector('#pose_loading').innerHTML = `<img src="assets/loading.svg" width="48" height="48">`

      var scene_id = document.querySelector('input[name="scene_id"]:checked').value;
      let assetUrl = new URL('./data/scene_mesh/' + scene_id + '.glb', document.URL)
      var prefix = document.getElementById("prefix");
      prefix.innerHTML = scene_id.replace("scannet_", "ScanNet: ").replace("3rscan_", "3RScan: ").replace("multiscan_", "MultiScan: ") + " ‚Äî object " + img.id[4];
      assetLoader1.load(assetUrl.href, gltf => {
        model1 = gltf.scene
        model1.name = "mesh"
        scene1.add(model1)
        document.querySelector('#pose_loading').innerHTML = ''

        img1.onclick = function() {populateDemo(img1)};
        img2.onclick = function() {populateDemo(img2)};
        img3.onclick = function() {populateDemo(img3)};

        var gqas = img.alt.split("[SEP]");
        var gs = gqas[0].split("[sep]");
        var qas = gqas[1].split("[Sep]");
        for (timeoutId of timeoutIds) {
            clearTimeout(timeoutId);
        }
        var delay = 0;
        var cond;
        // console.log(gs)
        for ([idx, g] of gs.entries()) {
          idx += 1;
          var grounding = document.getElementById("g"+idx);
          grounding.innerHTML = "";
          if (idx == 1) {
            cond = "";
          }
          else {
            cond = gs[idx-2];
          }
          timeoutIds.push(setTimeout(typeWriterGrounding, delay, g, 0, cond, idx));
          delay += g.length * 25;
        }
        for ([idx, qa] of qas.entries()) {
          idx += 1;
          qa = qa.split("[sep]")
          var question = document.getElementById("q"+idx);
          var answer = document.getElementById("a"+idx);
          question.innerHTML = "";
          answer.innerHTML = "";
          if (idx == 1) {
            cond = gs[2];
          }
          else {
            cond = qas[idx-2].split("[sep]")[1];
          }
          timeoutIds.push(setTimeout(typeWriterQuestion, delay, qa[0], 0, cond, idx));
          delay += qa[0].length * 25;
          cond = qa[0];
          timeoutIds.push(setTimeout(typeWriterAnswer, delay, qa[1], 0, cond, idx));
          delay += qa[1].length * 25;
        }
      }, undefined, (error) => {console.error(error)})
    }

    function typeWriterGrounding(txt, i, cond, idx) {
      var flag = false;
      if (idx == 1) {
        flag = true;
      }
      else if (document.getElementById("g"+String(idx-1)).innerHTML == cond) {
        flag = true;
      }
      if (flag == true) {
        if (i < txt.length) {
          document.getElementById("g"+idx).innerHTML += txt.charAt(i);
          i++;
          timeoutIds.push(setTimeout(typeWriterGrounding, 20, txt, i, cond, idx));
        }
      }
    }

    function typeWriterQuestion(txt, i, cond, idx) {
      var flag = false;
      if (idx == 1) {
        if (document.getElementById("g3").innerHTML == cond) {
          flag = true;
        }
      }
      else if (document.getElementById("a"+String(idx-1)).innerHTML == cond) {
        flag = true;
      }
      if (flag == true) {
        if (i < txt.length) {
          document.getElementById("q"+idx).innerHTML += txt.charAt(i);
          i++;
          timeoutIds.push(setTimeout(typeWriterQuestion, 20, txt, i, cond, idx));
        }
      }
    }

    function typeWriterAnswer(txt, i, cond, idx) {
      var flag = false;
      if (document.getElementById("q"+String(idx)).innerHTML == cond) {
        flag = true;
      }
      if (flag == true) {
        if (i < txt.length) {
          document.getElementById("a"+idx).innerHTML += txt.charAt(i);
          i++;
          timeoutIds.push(setTimeout(typeWriterAnswer, 20, txt, i, cond, idx));
        }
      }
    }

  </script>

  <script type="importmap">
    {
      "imports": {
        "vue": "https://unpkg.com/vue@3/dist/vue.esm-browser.js",
        "three": "https://unpkg.com/three@0.127.0/build/three.module.js",
        "three/addons/": "https://unpkg.com/three@0.127.0/examples/jsm/"
      }
    }
  </script>

  <!-- imported in PaLM-E -->
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-EVSTQN3/azprG1Anm3QDgpJLIm9Nao0Yz1ztcQTwFspd3yD65VohhpuuCOmLASjC" crossorigin="anonymous">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
  <link rel="stylesheet" href="https://github.com/palm-e/palm-e.github.io/blob/main/css/app.css">

  <link rel="stylesheet" href="https://github.com/palm-e/palm-e.github.io/blob/main/css/bootstrap.min.css">

  <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/js/bootstrap.bundle.min.js" integrity="sha384-MrcW6ZMFYlzcLA8Nl+NtUVF0sA7MsXsP1UyJoMp4YLEuNSfAP+JcXn/tWtIaxVXM" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
  
  <script src="https://github.com/palm-e/palm-e.github.io/blob/main/js/app.js"></script>

  <!-- imported in Nerfies -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./assets/logo.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="https://kit.fontawesome.com/69dc91e44b.js" crossorigin="anonymous"></script>

  <style>
    .grid {
      display: grid;
      gap: 1.5rem;
      grid-template-columns: 1fr;
    }
    @media (min-width: 768px) {
      .grid {
        grid-template-columns: repeat(2, 1fr);
      }
    }
  </style>

</head>

<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" target="_blank" href="https://siyuanhuang.com/">
        <span class="icon">
          <i class="fas fa-home"></i>
        </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
        More Research
        </a>
        <div class="navbar-dropdown">
        <a class="navbar-item" target="_blank" href="https://embodied-generalist.github.io/">
          LEO
        </a>
        <a class="navbar-item" target="_blank" href="https://msr3d.github.io/">
          MSR3D
        </a>
        <a class="navbar-item" target="_blank" href="https://scene-verse.github.io/">
          SceneVerse
        </a>
        <a class="navbar-item" target="_blank" href="https://pq3d.github.io/">
          PQ3D
        </a>
        <a class="navbar-item" target="_blank" href="https://arnold-benchmark.github.io/">
          ARNOLD
        </a>
        </div>
      </div>
    </div>
  </div>
</nav>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <img src="assets/logo.png" width="10%">
          <h1 class="title is-3 publication-title">Unveiling the Mist over 3D Vision-Language Understanding: Object-centric Evaluation with Chain-of-Analysis</h1>
          <h4 class="title is-4 publication-title">CVPR 2025</h4>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a target="_blank" href="https://huangjy-pku.github.io/">Jiangyong Huang</a><sup>1,2,‚ú∂</sup></span>
            <span class="author-block" style="margin-left: 1em;">
                <a target="_blank" href="https://buzz-beater.github.io/">Baoxiong Jia</a><sup>1,‚ú∂</sup></span>
            <span class="author-block" style="margin-left: 1em;">
                <a target="_blank" href="https://github.com/jetpackfirstme">Yan Wang</a><sup>1</sup></span>
            <span class="author-block" style="margin-left: 1em;">
              <a target="_blank" href="https://zhuziyu-edward.github.io/">Ziyu Zhu</a><sup>1,3</sup></span>
            <span class="author-block" style="margin-left: 1em;">
              <a target="_blank" href="https://github.com/Germany321">Xiongkun Linghu</a><sup>1</sup></span>
            <br/>
            <span class="author-block">
              <a target="_blank" href="https://liqing-ustc.github.io/">Qing Li</a><sup>1</sup></span>
            <span class="author-block" style="margin-left: 1em;">
              <a target="_blank" href="http://www.stat.ucla.edu/~sczhu/">Song-Chun Zhu</a><sup>1,2,3</sup></span>
            <span class="author-block" style="margin-left: 1em;">
              <a target="_blank" href="https://siyuanhuang.com/">Siyuan Huang</a><sup>1</sup></span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Beijing Institute for General Artificial Intelligence (BIGAI)</span>
            <br/>
            <span class="author-block"><sup>2</sup>Peking University</span>
            <span class="author-block" style="margin-left: 1em;"><sup>3</sup>Tsinghua University</span>
          </div>

          <p style="font-size: 0.9em; padding: 0.5em 0 0 0;">‚ú∂ indicates equal contribution</p>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- Arxiv Link. -->
              <span class="link-block">
                <a target="_blank" href="https://arxiv.org/abs/2503.22420"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a target="_blank" href="https://youtu.be/8hiGFwCQMjk"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Demo Link. -->
              <span class="link-block">
                <a target="_blank" href="https://huggingface.co/spaces/huangjy-pku/Beacon3D-Demo"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fa-regular fa-comments"></i>
                  </span>
                  <span>Demo</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a target="_blank" href="https://github.com/beacon-3d/beacon-3d"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Data Link. -->
              <span class="link-block">
                <a target="_blank" href="https://github.com/beacon-3d/beacon-3d/blob/main/data"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fa fa-database"></i>
                  </span>
                  <span>Data</span>
                  </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">

    <!-- Paper video -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <div class="publication-video">
          <iframe width="560" height="315" src="https://www.youtube.com/embed/8hiGFwCQMjk?si=_ryfpNopOCdhMyAX"
          title="YouTube video player" frameborder="0"
          allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
          referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
        </div>
      </div>
    </div>
    <!--/ Paper video -->

    <!-- Overview -->
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <h2 class="title is-4">Overview</h2>
        <img src="assets/teaser.png" alt="Illustrative overview of Beacon3D">
        <div class="content has-text-justified" style="margin-top: 5px;">
          <p>
            An illustration of Beacon3D, a novel benchmark for 3D grounding and question answering (QA) tasks.
            Beacon3D features an object-centric evaluation framework, with Grounding Chains (G-Chains) and Grounding-QA Chains (GQA-Chains) for each object.
            The evaluation adopts object-centric metrics to ensure robustness and utilizes chain-of-analysis for studies in task coherence.
            We also design various knowledge types such as class, appearance ("App."), geometry ("Geo."), spatial ("Spa."), and existence ("Exi.").
          </p>
        </div>

        <div class="content has-text-justified" style="width: 100%; margin: 0 auto;">
          <img src="assets/scenes.png" width="40%" alt="30 high-quality real 3D scenes in Beacon3D">
          <img src="assets/data_stats.png" width="59%" alt="Beacon3D data statistics">
          <p style="margin-top: -5px;">
            Beacon3D is built on 30 high-quality real 3D scenes meticulously selected from ScanNet, 3RScan, and MultiScan.
            The object-centric evaluation includes more than 800 objects and shows a diverse distribution of knowledge types in grounding and QA tasks.
          </p>
        </div>
      </div>
    </div>
    <!--/ Overview -->
    
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-4 has-text-centered">Summary</h2>
        <div class="grid grid-cols-1 md:grid-cols-2 gap-6">

          <div class="bg-gray-100 py-6 rounded-xl text-left shadow" style="padding-left: 3rem; padding-right: 2.5rem;">
            <h2 class="flex items-center text-xl font-semibold mb-4">
              ü§î<span class="ml-2" style="font-size: large; font-weight: bold;">Limitations of existing 3D-VL benchmarks</span>
            </h2>
            <ul class="list-none space-y-2">
              <li style="color: rgb(177, 92, 92); font-weight: bold;">üóÉÔ∏è&nbsp; Flawed test data</li>
              <li style="color: rgb(177, 92, 92); font-weight: bold;">üìä&nbsp; Insufficient evaluation metrics</li>
              <li style="color: rgb(177, 92, 92); font-weight: bold;">üîó&nbsp; Isolation of grounding and QA tasks</li>
            </ul>
          </div>

          <div class="bg-gray-100 p-6 rounded-xl text-left shadow">
            <h2 class="flex items-center text-xl font-semibold mb-4">
              üí°<span class="ml-2" style="font-size: large; font-weight: bold;">Highlights of the Beacon3D benchmark</span>
            </h2>
            <ul class="list-none space-y-2">
              <li style="color: rgb(84, 156, 79); font-weight: bold;">üóÉÔ∏è&nbsp; High-quality test data</li>
              <li style="color: rgb(84, 156, 79); font-weight: bold;">üìä&nbsp; Object-centric evaluation metrics</li>
              <li style="color: rgb(84, 156, 79); font-weight: bold;">üîó&nbsp; Grounding Chain and Grounding-QA Chain</li>
            </ul>
          </div>
        </div>
        <br/>

      </div>
    </div>

  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-4 has-text-centered">From Existing 3D-VL Benchmarks to Beacon3D</h2>

        <!-- Test data -->
        <div class="content has-text-justified">
          <p>
            <b style="color: rgb(177, 92, 92);">üóÉÔ∏è Flawed test data.</b>
            We observe notable data flaws including ambiguous referential texts in the grounding task,
            ambiguous questions and incomplete answers in the QA task. Such flawed test data could undermine the reliability of evaluation results.
          </p>
        </div>
        <div style="width: 100%; margin: -20px auto 15px auto;">
          <figure style="text-align: center;">
            <img src="assets/data_flaw.png" alt="Flawed test data in existing 3D-VL benchmarks">
            <figcaption style="font-size: 14px; color: #666;">
              <i><b>Flawed test data in existing 3D-VL benchmarks:</b> the top row shows grounding data and the bottom row shows QA data.</i>
            </figcaption>
          </figure>
        </div>

        <div class="content has-text-justified">
          <p>
            <b style="color: rgb(84, 156, 79);">üóÉÔ∏è Beacon3D: high-quality test data.</b>
            We establish detailed annotation guidelines, ensuring precise and natural language to address prior data flaws.
            The human study across different 3D-VL benchmarks highlights the quality of Beacon3D test data.
          </p>
        </div>
        <div style="display: flex; justify-content: center; gap: 20px; margin: -15px auto 15px auto; width: 100%;">
          <figure style="text-align: center;">
            <img src="assets/data_quality_grouding.png" width="41%" style="margin-left: 1%; margin-right: 2%;" alt="Human study on the quality of grounding data">
            <img src="assets/data_quality_qa.png" width="41%" style="margin-left: 2%; margin-right: 1%;" alt="Human study on the quality of QA data">
            <figcaption style="font-size: 14px; color: #666;">
              <i><b>Human study on the quality of test data:</b> the left shows grounding data and the right shows QA data.</i>
            </figcaption>
          </figure>
        </div>
        <!--/ Test data -->

        <!-- Evaluation metrics -->
        <div class="content has-text-justified">
          <p>
            <b style="color: rgb(177, 92, 92);">üìä Insufficient evaluation metrics.</b>
            We find that simple metrics, such as averaging accuracy over individual QA pairs,
            are vulnerable to pitfalls like visual ignorance and weak language robustness,
            falling short in capturing true model capability.
          </p>
        </div>
        <div style="display: flex; justify-content: center; margin: -15px auto 15px auto; width: 100%;">
          <figure style="text-align: center;">
            <img src="assets/visual_ignorance.png" width="100%" style="margin-left: 2%; margin-right: 0%; margin-bottom: 5px;" alt="Model pitfall: visual ignorance">
            <figcaption style="font-size: 14px; color: #666;"><i><b>Model pitfall: visual ignorance</b></i></figcaption>
          </figure>
          <figure style="text-align: center;">
            <img src="assets/language_robustness.png" width="91.5%" style="margin-left: 1%; margin-right: -3%; margin-bottom: 4px;" alt="Model pitfall: weak language robustness">
            <figcaption style="font-size: 14px; color: #666;"><i><b>Model pitfall: weak language robustness</b></i></figcaption>
          </figure>
        </div>

        <div style="display: flex; gap: 20px; align-items: flex-start;">
          <!-- Left: Text -->
          <div style="flex: 1;">
            <div>
              We present two pilot studies to show the vulnerability of existing evaluation metrics to two model pitfalls:
              <ul style="list-style-type: disc; padding-left: 20px;">
                <li>
                  Visual ignorance: finetuning blind LLMs on SQA3D yields unexpectedly high accuracy,
                  indicating the deficiency in evaluating the visual capability of 3D-VL models.
                </li>
                <li>
                  Language robustness: rephrasing language yields moderate and significant performance shifts in grounding and QA tasks, respectively,
                  indicating that current 3D-VL models are susceptible to language variations.</li>
              </ul>
            </div>
          </div>
        
          <!-- Right: Images -->
          <div style="flex: 1;">
            <!-- Top image -->
            <figure style="text-align: center; margin-top: 25px; margin-bottom: 25px;">
              <img src="assets/sqa_blind.png" alt="Performance of blind LLMs on SQA3D" style="width: 100%;">
              <figcaption style="font-size: 14px; color: #666;"><i><b>Performance of blind LLMs on SQA3D ( <sup>‚Ä†</sup> denotes 3D-VL model)</b></i></figcaption>
            </figure>
        
            <!-- Bottom two images side-by-side -->
            <figure style="text-align: center; margin-bottom: 10px;">
              <div style="display: flex; gap: 10px; margin-bottom: -2px;">
                <img src="assets/rephrase_grounding.png" alt="Effect of rephrasing on grounding task" style="width: 49%;">
                <img src="assets/rephrase_qa.png" alt="Effect of rephrasing on QA task" style="width: 50%;">
              </div>
              <figcaption style="font-size: 14px; color: #666;"><i><b>Effect of rephrasing language on grounding (left) and QA (right) tasks</b></i></figcaption>
            </figure>
          </div>
        </div>

        <div class="content has-text-justified" style="margin-top: 5px;">
          <p>
            <b style="color: rgb(84, 156, 79);">üìä Beacon3D: object-centric evaluation metrics.</b>
            In contrast to previous per-case average metrics, we design three diverse test cases per object and adopt object-centric metrics,
            which require the model to make correct predictions in all three cases.
          </p>
        </div>
        <div style="display: flex; justify-content: center; margin: -15px auto 15px auto; width: 100%;">
          <figure style="text-align: center;">
            <img src="assets/grounding_example.png" width="100%" style="margin-left: 0%; margin-right: 0%; margin-bottom: 5px;" alt="Data example: three grounding texts per object">
            <figcaption style="font-size: 14px; color: #666;"><i><b>Data example: three grounding texts per object</b></i></figcaption>
          </figure>
          <figure style="text-align: center;">
            <img src="assets/obj_metrics.png" width="84%" style="margin-left: 1%; margin-right: -3%; margin-bottom: 3.5px;" alt="Object-centric metrics">
            <figcaption style="font-size: 14px; color: #666;"><i><b>Case-centric metrics vs. Object-centric metrics</b></i></figcaption>
          </figure>
        </div>
        <!--/ Evaluation metrics -->

        <!-- Grounding-QA coherence -->
        <div class="content has-text-justified" style="margin-top: 5px;">
          <p>
            <b style="color: rgb(84, 156, 79);">üîó Beacon3D: Grounding Chain.</b>
            We organize the grounding data into Grounding Chains following a coarse-to-fine scheme,
            which helps assess performance coherence across different granularities and identify the boundary of grounding capability.
          </p>
        </div>
        <div style="width: 80%; margin: -5px auto 15px;">
          <video poster="" id="grounding_chain" autoplay muted loop height="100%">
            <source src="assets/g_chain.mp4" type="video/mp4">
          </video>
        </div>

        <div class="content has-text-justified" style="margin-top: 5px;">
          <div>
            <b style="color: rgb(84, 156, 79);">üîó Beacon3D: Grounding-QA Chain.</b>
            Beacon3D links QA data to grounding data via shared referential texts of the target object.
            Each question queries a specific aspect (<i>e.g.</i>, appearance) of the object,
            forming a Grounding-QA Chain that enables analysis of grounding-QA coherence.
            We identify two types of broken coherence:
            <ul style="list-style-type: disc; margin-top: 5px; margin-left: 3%;">
              <li>
                Type 1: model fails to answer the queried content but can recognize that in grounding task, showing a lack of QA skills.
              </li>
              <li>
                Type 2: model correctly answers the question but fails to ground the target object, indicating shortcut behavior in QA.
            </ul>
          </div>
        </div>
        <div style="width: 90%; margin: -25px auto 0px;">
          <video poster="" id="grounding_chain" autoplay muted loop height="100%">
            <source src="assets/gqa_chain.mp4" type="video/mp4">
          </video>
        </div>
        <!--/ Grounding-QA coherence -->

      </div>
    </div>


    <!-- Demo -->
    <div class="columns is-centered">
      <div class="column is-full-width" id="demo">
        <h2 class="title is-4 has-text-centered">Data Visualizer</h2>
        <p class="has-text-centered" style="margin-top: -10px; margin-bottom: 20px;">
          Select a scene and then click an image to visualize scene and object-centric data
        </p>
        <div class="columns is-centered" style="position: relative;">
          <div class="control" style="display: flex; flex-direction: row; align-items: center; justify-content: center;">
            <label class="radio">
              <input value="scannet_scene0050_00" type="radio" name="scene_id"> <b>ScanNet: scene0050_00</b>
            </label>

            <span style="width:4em"></span>

            <label class="radio">
              <input value="scannet_scene0616_00" type="radio" name="scene_id"> <b>ScanNet: scene0616_00</b>
            </label>

            <span style="width:4em"></span>

            <label class="radio">
              <input value="3rscan_634b2183" type="radio" name="scene_id"> <b>3RScan: 634b2183</b>
            </label>

            <span style="width:4em"></span>

            <label class="radio">
              <input value="multiscan_scene_00106_05" type="radio" name="scene_id"> <b>MultiScan: scene_00106_05</b>
            </label>
          </div>
        </div>

        <div class="row border rounded" style="padding-top: 10px; padding-bottom:10px;">
          <div class="col-md-7">
            <div id="pose_loading" style="position: absolute; left: 26.6%; top: 67.3%;"></div>
            <canvas id="webgl_pose"></canvas>
          </div>

          <div class="col-md-5" style="transform: translate(-4.5px,0)">
            <div class="row">
              <div class="col-md-4 col-sm-4 col-xs-12">
                <img id="obj_1" src="data/obj_view/scannet_scene0050_00_1.jpg" width="100%"
                  alt="Scene ‚Äî object 1"
                  onclick="populateDemo(this);">
              </div>

              <div class="col-md-4 col-sm-4 col-xs-12">
                <img id="obj_2" src="data/obj_view/scannet_scene0050_00_2.jpg" width="100%"
                  alt="Scene ‚Äî object 2"
                  onclick="populateDemo(this);">
              </div>
    
              <div class="col-md-4 col-sm-4 col-xs-12">
                <img id="obj_3" src="data/obj_view/scannet_scene0050_00_3.jpg" width="100%"
                  alt="Scene ‚Äî object 3"
                  onclick="populateDemo(this);">
              </div>
            </div>

            <div id="prefix">Scene ‚Äî object</div>
            <div id="g1">Grounding text</div>
            <div id="g2"></div>
            <div id="g3"></div>
            <div><p id="q1" style="white-space: pre-line">Question</p></div>
            <div><p id="a1" style="white-space: pre-line">Answer</p></div>
            <div><p id="q2" style="white-space: pre-line"></p></div>
            <div><p id="a2" style="white-space: pre-line"></p></div>
            <div><p id="q3" style="white-space: pre-line"></p></div>
            <div><p id="a3" style="white-space: pre-line"></p></div>

          </div>
        </div>

        <br>
        <br>

      </div>
    </div>
    <!--/ Demo -->


    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-4 has-text-centered">Results and Findings</h2>
        <!-- Metrics -->
        <div class="content has-text-justified">
          <p>
            <b>Metrics.</b>
            Object-centric metrics elicit a significant performance drop compared to per-case metrics,
            suggesting that current 3D-VL models lack a comprehensive understanding of objects and are susceptible to language variations.
          </p>
        </div>
        <div style="display: flex; justify-content: center; gap: 0; margin: -10px auto 20px auto; width: 100%;">
          <figure style="text-align: center;">
            <img src="assets/results_grounding.png" width="39.7%" style="margin-right: 2%;" alt="Model performance in grounding task">
            <img src="assets/results_qa.png" width="44%" style="margin-left: 2%;" alt="Model performance in QA task">
            <figcaption style="font-size: 14px; color: #666; margin-top: 4px;">
              <i><b>Model performance in grounding (left) and QA (right) tasks:</b> "Case" denotes per-case metrics and "Obj." denotes object-centric metrics.</i>
            </figcaption>
          </figure>
        </div>
        <!--/ Metrics -->

        <!-- Chain analysis -->
        <div class="content has-text-justified">
          <p>
            <b>Chain analysis: GQA-Chain.</b>
            We visualize four types of GQA-Chains and observe a limited proportion of good grounding-QA coherence.
            <i>R</i><sub>1</sub> and <i>R</i><sub>2</sub> measure the two types of broken coherence, both hovering around 50%.
            This reveals a substantial gap between the skills of grounding and QA, and frequent shortcut behavior in QA.
          </p>
        </div>
        <div style="width: 100%; margin: -5px auto 15px auto;">
          <figure style="text-align: center;">
            <img src="assets/chain_analysis_gqa.png" style="margin-left: -2%;" alt="Chain analysis across GQA-Chains">
            <figcaption style="font-size: 14px; color: #666; margin-top: -10px;">
              <i><b>GQA-Chain analysis:</b> distribution of four types of GQA-Chains (left) and two metrics for evaluating broken grounding-QA coherence (right).</i>
            </figcaption>
          </figure>
        </div>

        <div style="display: flex; gap: 20px; align-items: flex-start; margin-bottom: 10px;">
          <!-- Left: Text -->
          <div style="flex: 0.55; margin-top: 30px;">
            <b>Chain analysis: G-Chain.</b>
            Evaluation across coarse-to-fine G-Chains shows that fine-grained grounding is more challenging than coarse.
            The difficulty is pronounced when the model fails on coarse texts.
            As fine-grained grounding is crucial to solid QA performance,
            our chain analysis highlights the need to improve 3D-VL models in fine-grained grounding for better grounding-QA coherence.
          </div>
        
          <!-- Right: Images -->
          <div style="flex: 0.45; margin-left: -1%;">
            <!-- Top image -->
            <figure style="text-align: center;">
              <img src="assets/chain_analysis_g.png" alt="Chain analysis across G-Chains">
              <figcaption style="font-size: 14px; color: #666;">
                <i><b>G-Chain analysis:</b> distribution of four types of G-Chains.</i>
              </figcaption>
            </figure>
          </div>
        </div>
        <!--/ Chain analysis -->

        <!-- Additional insights -->
        <div class="content has-text-justified">
          <p>
            <b>Model insights.</b>
            Our evaluation indicates that incorporating LLMs into 3D-VL models weakens grounding capability and does not fundamentally enhance QA capability.
            This suggests the main bottleneck lies in 3D perception and VL alignment, rather than language modeling or reasoning ‚Äî LLMs' strength.
            Therefore, advancing 3D-VL models may rely more on stronger foundation models for 3D scene understanding than on leveraging LLMs.
          </p>
        </div>
        <!--/ Additional insights -->
      </div>
    </div>

  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title is-4 has-text-centered">BibTex</h2>
    <p style="margin-bottom: 10px;">If you find our work helpful, please consider citing us:</p>
    <pre><code class="language-bibtex">@inproceedings{huang2025unveiling,
  title={Unveiling the Mist over 3D Vision-Language Understanding: Object-centric Evaluation with Chain-of-Analysis},
  author={Huang, Jiangyong and Jia, Baoxiong and Wang, Yan and Zhu, Ziyu and Linghu, Xiongkun and Li, Qing and Zhu, Song-Chun and Huang, Siyuan},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2025}
}</code></pre>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
        <p>
            This website is licensed under a <a rel="license"
            href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
        </p>
        <p>
            Template borrowed from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.
        </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>


<!-- visualization code borrowed from SceneDiffuser -->
<script type="module">

  import * as THREE from 'three'
  import { OrbitControls } from 'three/addons/controls/OrbitControls.js'
  import { GLTFLoader } from 'three/addons/loaders/GLTFLoader.js'
  import { DRACOLoader } from 'three/addons/loaders/DRACOLoader.js';
  const dracoLoader = new DRACOLoader();
  // Set the path to where your Draco decoder files are (you can use CDN or self-host)
  dracoLoader.setDecoderPath('https://www.gstatic.com/draco/v1/decoders/'); 

  let canvas1 = document.querySelector('#webgl_pose')
  let scene1 = new THREE.Scene()
  let assetLoader1 = new GLTFLoader()
  assetLoader1.setDRACOLoader(dracoLoader);
  let model1

  let camera1 = new THREE.PerspectiveCamera(45, 1.618 / 1.0, 0.1, 100)
  // camera1.position.set(5.2, 3.9, -3.9)
  camera1.position.set(0, 7.0, -1.2)
  let grid1 = new THREE.GridHelper(30, 30)
  scene1.add(camera1)
  scene1.add(grid1)
  for (let i = 0; i <= 1; i++) {
    for (let k = 0; k <= 1; k++) {
      let spotLight = new THREE.SpotLight(0xAAAAAA)
      spotLight.position.set(50 * (i * 2 - 1), 100, 100 * (k * 2 - 1))
      scene1.add(spotLight)
    }
  }

  let controls1 = new OrbitControls(camera1, canvas1)
  controls1.enableZoom = true
  // controls2.enableDamping = true
  controls1.object.position.set(camera1.position.x, camera1.position.y, camera1.position.z)
  controls1.target = new THREE.Vector3(0, 0, 0.2)
  controls1.update()

  let renderer1 = new THREE.WebGLRenderer({
      canvas: canvas1,
      alpha: true,
  })
  renderer1.setPixelRatio(Math.min(window.devicePixelRatio, 2))
  renderer1.outputEncoding = THREE.sRGBEncoding
  renderer1.setAnimationLoop(() => {
    renderer1.render(scene1, camera1)
  });

  const radioButtons = document.querySelectorAll('input[name="scene_id"]')
  for (const radioButton of radioButtons) {
    radioButton.addEventListener('change', (e) => {
      var scene_id = radioButton.value;
      // console.log("scene_id", scene_id)
      for (const rb of radioButtons) {rb.disabled = true}

      var img1 = document.getElementById("obj_1");
      var img2 = document.getElementById("obj_2");
      var img3 = document.getElementById("obj_3");

      if (scene_id == "scannet_scene0050_00") {
        img1.src = "data/obj_view/scannet_scene0050_00_1.jpg"
        img1.alt = "Grounding 1: The armchair directly facing the desk." +
                   "[sep]Grounding 2: The armchair near the printer." +
                   "[sep]Grounding 3: Large chair." +
                   "[SEP]Question 1: What is in front of the large chair?" +
                   "[sep]Answer 1: Desk." +
                   "[Sep]Question 2: What color is the armchair near the printer?" +
                   "[sep]Answer 2: Brown." +
                   "[Sep]Question 3: What size is \"the armchair near the printer\" compared with the other chair?" +
                   "[sep]Answer 3: Large."

        img2.src = "data/obj_view/scannet_scene0050_00_2.jpg"
        img2.alt = "Grounding 1: The blue box next to the piano." +
                   "[sep]Grounding 2: Blue box." +
                   "[sep]Grounding 3: The lower box on the stool." +
                   "[SEP]Question 1: What color is the lower box on the stool?" +
                   "[sep]Answer 1: Blue." +
                   "[Sep]Question 2: Is there a similar box next to the blue box?" +
                   "[sep]Answer 2: Yes." +
                   "[Sep]Question 3: What size is the blue box compared with the adjacent box?" +
                   "[sep]Answer 3: Large."

        img3.src = "data/obj_view/scannet_scene0050_00_3.jpg"
        img3.alt = "Grounding 1: The backpack closer to chair." +
                   "[sep]Grounding 2: The black backpack far from piano." +
                   "[sep]Grounding 3: The backpack closer to desk." +
                   "[SEP]Question 1: What color is the backpack closer to chair?" +
                   "[sep]Answer 1: Black." +
                   "[Sep]Question 2: Is \"the black backpack far from piano\" on the couch?" +
                   "[sep]Answer 2: No." +
                   "[Sep]Question 3: Is there a trash can next to \"the backpack closer to desk\"?" +
                   "[sep]Answer 3: No."
      }

      else if (scene_id == "scannet_scene0616_00") {
        img1.src = "data/obj_view/scannet_scene0616_00_1.jpg"
        img1.alt = "Grounding 1: The picture closest to the window." +
                   "[sep]Grounding 2: The largest colorful picture." +
                   "[sep]Grounding 3: The colorful picture next to window." +
                   "[SEP]Question 1: Which is larger, \"the picture closest to the window\" or the window?" +
                   "[sep]Answer 1: Picture." +
                   "[Sep]Question 2: Is there a window next to the largest colorful picture?" +
                   "[sep]Answer 2: Yes." +
                   "[Sep]Question 3: Does \"the picture closest to the window\" have a black pattern?" +
                   "[sep]Answer 3: No."

        img2.src = "data/obj_view/scannet_scene0616_00_2.jpg"
        img2.alt = "Grounding 1: The gray chair next to the corner table." +
                   "[sep]Grounding 2: The chair closest to the radiator." +
                   "[sep]Grounding 3: The chair right under the painting by the window." +
                   "[SEP]Question 1: What color is the chair closest to the radiator?" +
                   "[sep]Answer 1: Gray." +
                   "[Sep]Question 2: What is above the chair closest to the radiator?" +
                   "[sep]Answer 2: Picture." +
                   "[Sep]Question 3: Look at the chair right under the painting by the window, is it near the trash bin?" +
                   "[sep]Answer 3: No."

        img3.src = "data/obj_view/scannet_scene0616_00_3.jpg"
        img3.alt = "Grounding 1: The trash bin closer to the armchair." +
                   "[sep]Grounding 2: The trash bin farther from the corner." +
                   "[sep]Grounding 3: White trash bin." +
                   "[SEP]Question 1: What is hanging above the white trash bin?" +
                   "[sep]Answer 1: Picture." +
                   "[Sep]Question 2: What color is the trash bin closer to the armchair?" +
                   "[sep]Answer 2: White." +
                   "[Sep]Question 3: Is there a table near the white trash bin?" +
                   "[sep]Answer 3: No."
      }

      else if (scene_id == "3rscan_634b2183") {
        img1.src = "data/obj_view/3rscan_634b2183_1.jpg"
        img1.alt = "Grounding 1: The tall cabinet." +
                   "[sep]Grounding 2: Cabinet in yellow and white." +
                   "[sep]Grounding 3: The cabinet next to the bed." +
                   "[SEP]Question 1: What color is upper part of the tall cabinet?" +
                   "[sep]Answer 1: Tan." +
                   "[Sep]Question 2: Is there a bed next to \"the cabinet in tan and white\"?" +
                   "[sep]Answer 2: Yes." +
                   "[Sep]Question 3: Is \"the cabinet next to the bed\" a tall one or short one?" +
                   "[sep]Answer 3: Tall."

        img2.src = "data/obj_view/3rscan_634b2183_2.jpg"
        img2.alt = "Grounding 1: Desk next to the sofa." +
                   "[sep]Grounding 2: Desk with chairs nearby." +
                   "[sep]Grounding 3: Large desk." +
                   "[SEP]Question 1: What color is the desk next to the sofa?" +
                   "[sep]Answer 1: White." +
                   "[Sep]Question 2: Is there a sofa next to the large desk?" +
                   "[sep]Answer 2: Yes." +
                   "[Sep]Question 3: Is the large desk close to door?" +
                   "[sep]Answer 3: No."

        img3.src = "data/obj_view/3rscan_634b2183_3.jpg"
        img3.alt = "Grounding 1: The small table." +
                   "[sep]Grounding 2: The table under the small whiteboard." +
                   "[sep]Grounding 3: The table beside a tall cabinet." +
                   "[SEP]Question 1: What is directly above the small table?" +
                   "[sep]Answer 1: Whiteboard." +
                   "[Sep]Question 2: What color is the surface of the small table?" +
                   "[sep]Answer 2: White." +
                   "[Sep]Question 3: Which is taller, \"the table under the small whiteboard\" or the adjacent cabinet?" +
                   "[sep]Answer 3: Cabinet."
      }

      else if (scene_id == "multiscan_scene_00106_05") {
        img1.src = "data/obj_view/multiscan_scene_00106_05_1.jpg"
        img1.alt = "Grounding 1: Yellow short commode." +
                   "[sep]Grounding 2: The short commode in front of door." +
                   "[sep]Grounding 3: The wide commode opposite the monitor." +
                   "[SEP]Question 1: What color is the short commode in front of door?" +
                   "[sep]Answer 1: Yellow." +
                   "[Sep]Question 2: Is there another commode next to the yellow short commode?" +
                   "[sep]Answer 2: Yes." +
                   "[Sep]Question 3: Look at the wide commode opposite the monitor, is it taller than the adjacent commode?" +
                   "[sep]Answer 3: No."

        img2.src = "data/obj_view/multiscan_scene_00106_05_2.jpg"
        img2.alt = "Grounding 1: The chair closest to the PC." +
                   "[sep]Grounding 2: The chair in front of a desk." +
                   "[sep]Grounding 3: The blue chair far from bed." +
                   "[SEP]Question 1: What is on the chair closest to the pc?" +
                   "[sep]Answer 1: Pillow." +
                   "[Sep]Question 2: What is behind \"the blue chair far from bed\"?" +
                   "[sep]Answer 2: Desk." +
                   "[Sep]Question 3: What color is the chair in front of a table?" +
                   "[sep]Answer 3: Blue."

        img3.src = "data/obj_view/multiscan_scene_00106_05_3.jpg"
        img3.alt = "Grounding 1: The chair next to the window." +
                   "[sep]Grounding 2: Brown chair." +
                   "[sep]Grounding 3: The chair farthest from the door." +
                   "[SEP]Question 1: What color is \"the chair next to the window\"?" +
                   "[sep]Answer 1: Brown." +
                   "[Sep]Question 2: Is the brown chair far from the window?" +
                   "[sep]Answer 2: No." +
                   "[Sep]Question 3: Is the brown chair far from the door?" +
                   "[sep]Answer 3: Yes."
      }

      for (const rb of radioButtons) {rb.disabled = false}
    })
  }
  radioButtons[0].click()

  // resize renderers
  function resizeRenderers() {
    let content_width = document.querySelector('#demo').offsetWidth
    let canvas_width = content_width * 6.7 / 12;
    let canvas_height = content_width / 1.618 * 6.7 / 12;
    renderer1.setSize(canvas_width, canvas_height)

    // Set canvas size
    canvas_height = canvas_height * 1.4;
    renderer1.setSize(canvas_width, canvas_height);
    // Update camera aspect ratio
    camera1.aspect = canvas_width / canvas_height;
    camera1.updateProjectionMatrix();
  }
  window.addEventListener('resize', () => {
    resizeRenderers()
  })
  resizeRenderers()

  // clip room roof
  const plane = new THREE.Plane(new THREE.Vector3(0, -1, 0), 2.4);
  renderer1.clippingPlanes = [plane];

  window.model1 = model1;
  window.scene1 = scene1;
  window.assetLoader1 = assetLoader1;

</script>


</html>
